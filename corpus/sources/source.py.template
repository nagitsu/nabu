import re
import requests

from datetime import datetime
from lxml import html


SOURCE_DOMAIN = '<DOMAIN>'
DOCUMENT_URL = '<DOCUMENT-URL-WITH-{}>'


def get_missing_ids(existing_ids):
    response = requests.get(<BASE-URL>)
    link_re = re.compile(r'.*/noticia/(\d+)/')

    root = html.fromstring(response.content)
    links = root.xpath("//a/@href")
    ids = []
    for link in links:
        m = link_re.match(link)
        if m:
            ids.append(int(m.group(1)))

    last_id = max(ids)

    scraped_ids = set(map(int, existing_ids))
    missing_ids = map(str, set(range(last_id)) - set(scraped_ids))

    return missing_ids


def get_content(response):
    # Check if the response is valid.
    if response.status_code == 404:
        return {'outcome': 'notfound'}
    elif response.status_code >= 400:
        return {'outcome': 'timeout'}

    root = html.fromstring(response.content)

    try:
        content = <PARSED-DATA>
    except:
        return {'outcome': 'unparseable'}

    result = {
        'outcome': 'success',
        'content': content,
        'tags': [<TAGS>]
    }

    return result


def _parse_date(date):
    """
    Example: "+ - 25.01.2015, 16:10 hs".
    """
    match = re.match('[^\d]*(\d+)\.(\d+)\.(\d+),\s*(\d+):(\d+)', date, flags=re.UNICODE)
    if match:
        day = int(match.group(1))
        month = int(match.group(2))
        year = int(match.group(3))
        hour = int(match.group(4))
        minute = int(match.group(5))

        return datetime(year, month, day, hour, minute)


def get_metadata(response):
    root = html.fromstring(response.content)

    metadata = {}
    try:
        metadata['title'] = <TITLE>
    except:
        pass

    try:
        raw_date = <DATE-STRING>
        date = _parse_date(raw_date.strip())
        if date:
            metadata['date'] = date
    except:
        pass

    return metadata
